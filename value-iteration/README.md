# Implementing Value Iteration - Shortest path in a maze

The problem statement has two parts:

- Convert a given MDP to Value function and optimal policy values
- Convert a maze to an MDP and use the solution of the first part to find the shortest path in the maze.

A complete description of the problem statement can be found [here](https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs747-a2020/pa-2/programming-assignment-2.html). This code only includes the implementation of the Value Iteration algorithm. Other two algorithms mentioned in the above assignment, Howard's policy Iteration and Linear Programming are not included.

### Getting started
Visit the above link and familiarise yourself with the format in which MDPs are represented in this problem. You do not need to download the files on the website if you have already cloned the repo (since they are identical).

In addition to the files available on the website, this folder has an additional sub-folder __solution__. This contains a solution to both parts of the problem statement.

The files can be executed in the following manner, in the given order.

1) encoder.py
~~~
python encoder.py --grid [grid file path]
~~~
All these are compulsory arguments. [grid file path] must be replaced by path to maze files available under [value-iteration/data/maze](value-iteration/data/maze).

The mdp file generated by this code is output to mdpfile.txt in the folder value-iteration/solution. An example of such a file is [mdpfile.txt](value-iteration/mdpfile.txt).

2) planner.py
~~~
python planner.py --mdp [mdp file path] --algorithm vi --outf [output file path]
~~~
All these are compulsory arguments. [mdp file path] must be replaced either by path to mdp files available under [value-iteration/data/mdp](value-iteration/data/mdp) or an mdpfile.txt generated from a maze using encoder.py as described above. Note that the argument --outf is not present in the instructions link but is required to run this solution.

The generated output file contains pairs of values of the value function and optimal policy for each state in the MDP. An example of such a file is [value_and_policy_file.txt](value-iteration/value_and_policy_file.txt).


3) decoder.py
~~~
python decoder.py --grid [grid file path] --value_policy [value_and_policy_file path]
~~~
All these are compulsory arguments. [grid file path] must be replaced by path to maze files available under [value-iteration/data/maze](value-iteration/data/maze). [value_and_policy_file path] must be replaced by path to the output file of planner.py (which you must've chosen as argument --outf). This code prints the optimal path in the form of directions taken at each step from start to finish for the chosen grid. Note this must be the same grid that was used to generate the mdp with encoder.py and used to generate the value and policy file with planner.py.

Alternately the output can be redirected to another file. This can be used to visualise the solution. The details for this can be found in the same [link](https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs747-a2020/pa-2/programming-assignment-2.html) given above.

Note: This code does not work very well for large grid sizes due to memory inefficient storage of probability and reward functions. However the solution is logically correct and works well for small grids. Larger grids may require unreasonably long computation time. This is owing to my lack of python experience at that time :P. Later stages of the project have more efficient representations with much lesser redundancy.
